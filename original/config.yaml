API:
  LARGE_API_KEY: key-here
  LARGE_MODEL: qwen3:30b-12k #qwq-12k #gemma3-27b-QAT-12k:latest #mistral-small3.1-12k #qwen2.5:32b-12k #mistral-small:24b-12k #deepseek-r1:32b-12k #llama3.3-12k:latest #deepseek-r1:8b-12k #deepseek-r1:14b-12k #llama3.3-12k:latest #deepseek-r1:32b-12k #marco-o1-12k #deepseek-r1:8b-12k # #gemma2:27b-12k #llama3.3-12k:latest #Llama3.1-Nemotron-51B-Instruct-12k:latest #qwq-12k:latest #llama3:latest-12k #mixtral:latest-12k #mistral-small-12k #mistral-small:24b-12k #deepseek-r1:7b-12k #qwq-12k:latest
  LARGE_BASE_URL: http://localhost:11434/v1
  LARGE_MODE: api
  #SMALL_MODEL: nemotron-mini:latest
  SMALL_MODEL:  qwen3:8b-12k #llama3.1:latest-12k # #marco-o1-12k #gemma2:9b-12k #phi3.5-12k:latest #llama3:latest-12k #llama3.2:1b-12k  #llama3.2-12k #command-r7b-12k #granite3.1-dense-12k #mistral-small 
  SMALL_BASE_URL: http://localhost:11434/v1
  SMALL_API_KEY: key-here
  SMALL_MODE: api
HUGGINGFACE:
  HUB_PATH: yourusername/your-path-here
  PRIVATE: False
  PUSH_TO_HUB: False
PATH:
  DEFAULT_PROMPTS: ./prompts
  INPUT: ./input
  OUTPUT: ./output
  PROMPTS:  ./prompt_overrides/Reasoning_Prompts #./prompts  #
PHASE:
  PHASE_INDEX: 3
  WORK_IN_PHASES: false
SKIP:
  ANSWER_RELEVANCY_CHECK: False
  REPAIR_QA_TUPLES: False
  FILTER_CHUNKS: False
  QUESTION_CHECK: False
  CONVERSATION_GENERATION: False
SYSTEM:
  CHUNK_SIZE: 1900
  COMPLETION_MODE: False
  CONCURRENCY_LIMIT: 50
  CONVERSATION_INSTRUCTIONS: For this conversation, you are generating a chat between
    a generalist, generic AI assistant, and a human.
  DOUBLE_CHECK_COUNTER: 3
  DO_NOT_USE_SYSTEM_PROMPTS: false #changed to true to match ollama template MFJ
  FINAL_ASSISTANT_PROMPTS_NO_RAG: [
  'You are a helpful AI assistant.',
  'You are A VASTLY intelligent ARTIFICIAL INTELLIGENCE with DOMAIN-EXPERT KNOWLEDGE from a variety of fields.
  
  USE your knowledge to be helpful and truthfully answer questions about the world.',
  "u are ai asstant plz answr questions"] # a wide variety of system prompts helps the AI learn better. What, you expect your users to spell things right?
  FINAL_ASSISTANT_PROMPTS_RAG: [
  'You are a helpful AI assistant. Some knowledge:
  
  {data}',
  
  '{data}
  
  You are an AI domain expert. Answer questions',
  'You are an AI with vast knowledge. Here is some potentially-relevant context:
  
  {data}

  Answer questions according to your knowledge.']
  STOP: True
  SUBSET_SIZE: 20
  USE_FILENAMES: False
  USE_SUBSET: false #changed to false to match ollama template MFJ
  RAG_FAILURE_PERCENTAGE: 0.1 # How much of the RAG data has the wrong chunk retrieved deliberately? To train it to answer correctly even if wrong facts are shown to it. We will need another dataset thing for making data where the question asks something that is not present and the rag retrieves something irrelevant obbviously and it is supposed to say "I don't know" or something.
SCRAPING:
  USE_GUTENBERG: False
  START_URL: "https://www.gutenberg.org/ebooks/bookshelf/57"
  MAX_BOOKS: 5
  MAX_FAILURES: 5
